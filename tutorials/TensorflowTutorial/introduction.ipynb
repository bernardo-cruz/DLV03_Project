{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import constant, Variable\n",
    "\n",
    "import numpy as np\n",
    "import skimage as skimage\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Algebra Operations in Tensorflow\n",
    "\n",
    "Throughout this course, we will use tensorflow version 2.6.0 and will exclusively import the submodules needed to complete each exercise. \n",
    "This will usually be done for you, but you will do it in this exercise by importing constant from tensorflow.\n",
    "After you have imported constant, you will use it to transform a numpy array, credit_numpy, into a tensorflow constant, credit_constant. \n",
    "This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters.\n",
    "\n",
    "Note that tensorflow 2 allows you to use data as either a numpy array or a tensorflow constant object. \n",
    "\n",
    "Using a constant will ensure that any operations performed with that object are done in tensorflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-06 23:32:25.908404: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "credit_numpy = np.array([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12]\n",
    "    ])\n",
    "credit_constant = constant(credit_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining variables\n",
    "Unlike a constant, a variable's value can be modified. This will be useful when we want to train a model by updating its parameters.\n",
    "\n",
    "Let's try defining and printing a variable. We'll then convert the variable to a numpy array, print again, and check for differences. Note that Variable(), which is used to create a variable tensor, has been imported from tensorflow and is available to use in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A1:  <tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n",
      "\n",
      " B1:  [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Define the 1-dimensional variable A1\n",
    "A1 = Variable([1, 2, 3, 4])\n",
    "\n",
    "# Print the variable A1\n",
    "print('\\n A1: ', A1)\n",
    "\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = A1.numpy()\n",
    "\n",
    "# Print B1\n",
    "print('\\n B1: ', B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import ones, constant, add, matmul, multiply, Variable, reduce_sum, ones_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing element-wise multiplication\n",
    "Element-wise multiplication in TensorFlow is performed using two tensors with identical shapes. This is because the operation multiplies elements in corresponding positions in the two tensors. An example of an element-wise multiplication, denoted by the  symbol, is shown below:\n",
    "\n",
    "  \n",
    "$$\\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 1 \\\\ 2 & 5 \\end{bmatrix} = \\begin{bmatrix} 3 & 2 \\\\ 4 & 5 \\end{bmatrix}$$  \n",
    " \n",
    "\n",
    "In this exercise, you will perform element-wise multiplication, paying careful attention to the shape of the tensors you multiply. Note that multiply(), constant(), and ones_like() have been imported for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " C1: [1 2 3 4]\n",
      "\n",
      " C23: [[1 2 3]\n",
      " [1 6 4]]\n"
     ]
    }
   ],
   "source": [
    "# Define tensors A1 and A23 as constants\n",
    "A1 = constant([1, 2, 3, 4])\n",
    "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "# Define B1 and B23 to have the correct shape\n",
    "B1 = ones_like(A1)\n",
    "B23 = ones_like(A23)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "C1 = multiply(A1,B1)\n",
    "C23 = multiply(A23,B23)\n",
    "\n",
    "# Print the tensors C1 and C23\n",
    "print('\\n C1: {}'.format(C1.numpy()))\n",
    "print('\\n C23: {}'.format(C23.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions with matrix multiplication\n",
    "In later chapters, you will learn to train linear regression models. This process will yield a vector of parameters that can be multiplied by the input data to generate predictions. In this exercise, you will use input data, features, and a target vector, bill, which are taken from a credit card dataset we will use later in the course.\n",
    " \n",
    "$$features = \\begin{bmatrix} 2 & 24 \\\\ 2 & 26 \\\\ 2 & 57 \\\\ 1 & 37 \\end{bmatrix}, bill = \\begin{bmatrix} 3913 \\\\ 2682 \\\\ 8617 \\\\ 64400 \\end{bmatrix}, params = \\begin{bmatrix} 1000 \\\\ 150\\end{bmatrix}$$\n",
    "\n",
    "The matrix of input data, features, contains two columns: education level and age. The target vector, bill, is the size of the credit card borrower's bill.\n",
    "\n",
    "Since we have not trained the model, you will enter a guess for the values of the parameter vector, params. You will then use matmul() to perform matrix multiplication of features by params to generate predictions, billpred, which you will compare with bill. Note that we have imported matmul() and constant()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[[-1687]\n",
      " [-3218]\n",
      " [-1933]\n",
      " [57850]]\n"
     ]
    }
   ],
   "source": [
    "# Import operators from tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import constant, Variable, ones, matmul, multiply, reduce_sum\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define features, params, and bill as constants\n",
    "features = constant([\n",
    "    [2, 24], \n",
    "    [2, 26], \n",
    "    [2, 57], \n",
    "    [1, 37],\n",
    "    ])\n",
    "\n",
    "params = constant([\n",
    "    [1000], \n",
    "    [150],\n",
    "    ])\n",
    "\n",
    "bill = constant([\n",
    "    [3913], \n",
    "    [2682], \n",
    "    [8617], \n",
    "    [64400],\n",
    "    ])\n",
    "\n",
    "# Compute billpred using features and params\n",
    "billpred = matmul(features,params)\n",
    "\n",
    "# Compute and print the error\n",
    "error = bill - billpred\n",
    "print(error.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing over tensor dimensions\n",
    "You've been given a matrix, wealth. This contains the value of bond and stock wealth for five individuals in thousands of dollars.\n",
    "\n",
    "$$wealth = \\begin{bmatrix} 11 & 50 \\\\ 7 & 2 \\\\ 4 & 60 \\\\ 3 & 0 \\\\ 25 & 10 \\end{bmatrix}$$\n",
    "\n",
    "The first column corresponds to bonds and the second corresponds to stocks. Each row gives the bond and stock wealth for a single individual. Use wealth, reduce_sum(), and .numpy() to determine which statements are correct about wealth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First column | Second column\n",
      " [ 50 122]\n"
     ]
    }
   ],
   "source": [
    "wealth = constant([\n",
    "    [11,50],\n",
    "    [7,2],\n",
    "    [4,60],\n",
    "    [3,0],\n",
    "    [25,10],\n",
    "])\n",
    "\n",
    "print(\n",
    "    'First column | Second column\\n',\n",
    "    reduce_sum(wealth, axis=0).numpy()\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Operations\n",
    "\n",
    "- We have learned the following basic operations using Tensorflow\n",
    "  - add(): Elemenwise summation of two tensors\n",
    "  - multiply(): Element-wise multiplication of two tensors\n",
    "  - matmul(): Matrix multiplication of two tensors\n",
    "  - reduce_sum(): Sums over all dimensions of a tensor.\n",
    "\n",
    "- Now we explore more advance operations such as:\n",
    "  - gradient(): Computes the gradient of a function with respect to one or more variables\n",
    "  - reshape(): Reshapes a tensor\n",
    "  - random(): Generates random values\n",
    "\n",
    "The aim of this operations is for example to find local minima and maxima of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n"
     ]
    }
   ],
   "source": [
    "## Example \n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define x\n",
    "x = tf.Variable(-1.0)\n",
    "\n",
    "# Define y within instance of GradientTape\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.multiply(x,x)\n",
    "    \n",
    "# Evaluate the gradients of y with respect to x at x = -1\n",
    "g = tape.gradient(y,x)\n",
    "print(g.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping tensors\n",
    "Later in the course, you will classify images of sign language letters using a neural network. In some cases, the network will take 1-dimensional tensors as inputs, but your data will come in the form of images, which will either be either 2- or 3-dimensional tensors, depending on whether they are grayscale or color images.\n",
    "\n",
    "The figure below shows grayscale and color images of the sign language letter A. The two images have been imported for you and converted to the numpy arrays gray_tensor and color_tensor. Reshape these arrays into 1-dimensional vectors using the reshape operation, which has been imported for you from tensorflow. Note that the shape of gray_tensor is 28x28 and the shape of color_tensor is 28x28x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_tensor = np.array([[187, 189, 190, 192, 195, 198, 198, 198, 200, 200, 201, 205, 206,\n",
    "        203, 206, 206, 206, 207, 209, 207, 205, 205, 208, 208, 206, 207,\n",
    "        206, 206],\n",
    "       [189, 191, 193, 195, 197, 199, 200, 199, 200, 201, 202, 206, 206,\n",
    "        208, 204, 204, 210, 209, 208, 208, 207, 206, 208, 208, 207, 207,\n",
    "        207, 207],\n",
    "       [189, 192, 195, 198, 198, 201, 202, 203, 205, 206, 205, 209, 207,\n",
    "        204, 211, 210, 205, 208, 211, 208, 206, 207, 209, 210, 210, 208,\n",
    "        210, 210],\n",
    "       [191, 192, 195, 197, 199, 199, 204, 201, 203, 208, 206, 207, 209,\n",
    "        207, 213, 208, 209, 211, 221, 205, 204, 239, 182, 212, 213, 212,\n",
    "        209, 209],\n",
    "       [193, 195, 195, 199, 201, 201, 203, 205, 206, 216, 223, 203, 207,\n",
    "        239, 225, 193, 188, 230, 232, 195, 176, 239, 191, 205, 215, 212,\n",
    "        211, 213],\n",
    "       [194, 196, 199, 200, 206, 202, 205, 207, 196, 255, 238, 197, 160,\n",
    "        235, 226, 191, 146, 190, 226, 201, 160, 228, 211, 162, 215, 214,\n",
    "        216, 213],\n",
    "       [195, 198, 201, 202, 202, 203, 232, 215, 197, 246, 230, 215, 153,\n",
    "        235, 221, 205, 162, 181, 224, 210, 166, 202, 209, 163, 190, 213,\n",
    "        211, 213],\n",
    "       [196, 198, 201, 208, 202, 243, 240, 215, 166, 246, 233, 223, 171,\n",
    "        235, 212, 214, 165, 206, 224, 199, 152, 126, 206, 199, 170, 165,\n",
    "        214, 215],\n",
    "       [198, 201, 197, 204, 189, 247, 244, 230, 206, 246, 235, 220, 188,\n",
    "        235, 211, 218, 142, 227, 223, 176, 152,  74, 192, 206, 200, 145,\n",
    "        177, 215],\n",
    "       [200, 203, 206, 206, 236, 254, 254, 233, 215, 205, 242, 219, 185,\n",
    "        234, 230, 223, 131, 201, 209, 156, 141,  83, 175, 220, 196, 157,\n",
    "        185, 222],\n",
    "       [201, 200, 201, 193, 253, 231, 245, 246, 209, 159, 241, 214, 176,\n",
    "        219, 234, 212, 133, 132, 175, 149, 109, 100, 225, 226, 209, 147,\n",
    "        219, 221],\n",
    "       [202, 203, 203, 196, 253, 209, 241, 233, 194, 150, 234, 204, 174,\n",
    "        160, 208, 189, 146, 101, 172, 145,  76, 195, 230, 226, 194, 121,\n",
    "        227, 224],\n",
    "       [204, 203, 210, 245, 251, 222, 207, 198, 152, 112, 207, 171, 163,\n",
    "         97, 163, 154, 122, 105, 175, 169, 175, 227, 226, 206, 154, 147,\n",
    "        219, 225],\n",
    "       [204, 205, 201, 250, 246, 217, 167, 204, 146, 116, 192, 170, 161,\n",
    "         78, 151, 165, 115, 181, 228, 225, 223, 215, 203, 181, 144, 202,\n",
    "        220, 227],\n",
    "       [205, 207, 198, 252, 254, 228, 198, 185, 162, 128, 202, 194, 144,\n",
    "         64, 135, 155, 237, 241, 237, 226, 211, 177, 179, 155, 142, 227,\n",
    "        227, 228],\n",
    "       [210, 211, 208, 255, 252, 240, 219, 187, 168, 148, 187, 202, 151,\n",
    "        103, 192, 246, 253, 244, 233, 221, 199, 179, 157, 154, 116, 234,\n",
    "        228, 231],\n",
    "       [208, 211, 209, 254, 254, 248, 231, 216, 193, 175, 178, 201, 208,\n",
    "        240, 253, 254, 249, 238, 222, 206, 185, 160, 143, 143, 214, 231,\n",
    "        230, 230],\n",
    "       [209, 212, 205, 254, 254, 252, 241, 229, 217, 187, 207, 224, 249,\n",
    "        253, 251, 250, 242, 228, 206, 183, 166, 150, 143, 172, 229, 234,\n",
    "        235, 230],\n",
    "       [208, 211, 206, 254, 254, 255, 249, 238, 231, 211, 213, 230, 250,\n",
    "        254, 252, 246, 233, 217, 188, 164, 150, 143, 120, 235, 231, 230,\n",
    "        231, 231],\n",
    "       [209, 213, 211, 253, 255, 255, 252, 244, 233, 222, 217, 224, 246,\n",
    "        251, 242, 234, 225, 195, 173, 153, 134, 116, 225, 232, 235, 232,\n",
    "        233, 233],\n",
    "       [209, 214, 214, 246, 254, 253, 252, 240, 224, 214, 213, 217, 233,\n",
    "        233, 230, 214, 199, 190, 150, 145, 127, 201, 233, 234, 232, 234,\n",
    "        233, 234],\n",
    "       [211, 215, 215, 243, 254, 254, 245, 232, 221, 208, 213, 218, 225,\n",
    "        223, 206, 195, 169, 157, 132, 126, 170, 238, 234, 235, 234, 234,\n",
    "        234, 234],\n",
    "       [214, 216, 217, 209, 254, 250, 236, 229, 212, 197, 206, 210, 221,\n",
    "        210, 196, 170, 148, 140, 118, 134, 240, 235, 234, 235, 235, 236,\n",
    "        235, 236],\n",
    "       [186, 175, 180, 150, 156, 158, 144, 124, 132, 134, 148, 153, 150,\n",
    "        146, 137, 134, 126, 109, 114, 235, 237, 234, 238, 236, 236, 236,\n",
    "        236, 237],\n",
    "       [145, 135, 137, 134, 122, 136, 112,  95,  94,  90,  93,  65,  60,\n",
    "         66,  61,  66,  58,  66,  80, 164, 247, 235, 236, 237, 239, 237,\n",
    "        237, 235],\n",
    "       [140, 146, 136, 132, 129, 134, 100, 103, 100, 100,  87,  64,  66,\n",
    "         65,  57,  57,  61,  61,  64,  65, 177, 242, 238, 238, 239, 238,\n",
    "        238, 238],\n",
    "       [141, 146, 140, 131, 130, 136,  93,  97, 102,  96,  78,  71,  68,\n",
    "         64,  60,  61,  60,  55,  58,  48, 254, 238, 240, 239, 238, 237,\n",
    "        237, 238],\n",
    "       [146, 143, 137, 138, 129, 113,  94,  98, 101,  87,  75,  70,  68,\n",
    "         63,  60,  58,  56,  57,  63,  81, 237, 237, 240, 240, 239, 240,\n",
    "        240, 240]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the grayscale image tensor into a vector\n",
    "gray_vector = tf.reshape(gray_tensor, (28*28*1, 1)) # Shape of image vector is 28x28x1 (for grayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing with gradient\n",
    "You are given a loss function, , which you want to minimize. You can do this by computing the slope using the GradientTape() operation at different values of x. If the slope is positive, you can decrease the loss by lowering x. If it is negative, you can decrease it by increasing x. This is how gradient descent works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "x = np.arange(-2,2,0.1)\n",
    "plt.plot(x,x**2)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you will use a high level tensorflow operation to perform gradient descent automatically. In this exercise, however, you will compute the slope at x values of -1, 1, and 0. The following operations are available: GradientTape(), multiply(), and Variable()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n",
      "2.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def compute_gradient(x0):\n",
    "    # Define x as a variable with an initial value of x0\n",
    "    x = Variable(x0)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "        y = multiply(x,x)\n",
    "    # Return the gradient of y with respect to x\n",
    "    return tape.gradient(y, x).numpy()\n",
    "\n",
    "# Compute and print gradients at x = -1, 1, and 0\n",
    "print(compute_gradient(-1.0))\n",
    "print(compute_gradient(1.0))\n",
    "print(compute_gradient(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with image data\n",
    "You are given a black-and-white image of a letter, which has been encoded as a tensor, _letter_. You want to determine whether the _letter_ is an X or a K. You don't have a trained neural network, but you do have a simple model, _model_, which can be used to classify _letter_.\n",
    "\n",
    "The 3x3 tensor, _letter_, and the 1x3 tensor, _model_, are available in the Python shell. You can determine whether _letter_ is a K by multiplying _letter_ by _model_, summing over the result, and then checking if it is equal to 1. As with more complicated models, such as neural networks, _model_ is a collection of weights, arranged in a tensor.\n",
    "\n",
    "Note that the functions reshape(), matmul(), and reduce_sum() have been imported from tensorflow and are available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Letter:\n",
      " [[1 0 1]\n",
      " [1 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "Model:\n",
      " [[ 1  0 -1]]\n",
      "\n",
      "Model after reshape:\n",
      " tf.Tensor(\n",
      "[[ 1]\n",
      " [ 0]\n",
      " [-1]], shape=(3, 1), dtype=int64)\n",
      "\n",
      "Output of matmul(letter,model):\n",
      " tf.Tensor(\n",
      "[[0]\n",
      " [1]\n",
      " [0]], shape=(3, 1), dtype=int64)\n",
      "\n",
      " [1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import reshape, matmul, reduce_sum\n",
    "\n",
    "letter = np.array([[1,0,1],[1,1,0],[1,0,1]])\n",
    "print('\\nLetter:\\n',letter)\n",
    "model = np.array([[1,0,-1]])\n",
    "print('\\nModel:\\n',model)\n",
    "\n",
    "# Reshape model from a 1x3 to a 3x1 tensor\n",
    "model = reshape(model, (3*1, 1))\n",
    "print('\\nModel after reshape:\\n',model)\n",
    "\n",
    "# Multiply letter by model\n",
    "output = matmul(letter, model)\n",
    "print('\\nOutput of matmul(letter,model):\\n',output)\n",
    "\n",
    "# Sum over output and print prediction using the numpy method\n",
    "prediction = reduce_sum(output,axis=0)\n",
    "print('\\n',prediction.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data\n",
    "## Data can be imported using tensorflow\n",
    "- Usefull for managing complex pipelines\n",
    "- Not necessary for this chapter  \n",
    "## Simpler option used in this chapter\n",
    "- Import data using pandas\n",
    "- Convert data to numpy array\n",
    "- Use in _tensorflow_ without modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First Approach using Pandas and Numpy! \n",
    "# Load data from csv\n",
    "housing = pd.read_csv('kc_house_data.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert to numpy array\n",
    "#housing = np.array(housing)\n",
    "\n",
    "# Convert price column to float32\n",
    "price = np.array(housing['price'], dtype=np.float32)\n",
    "\n",
    "# Convert waterfront column to boolean\n",
    "waterfront = np.array(housing['waterfront'], dtype=np.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second approach using Tensorflow!\n",
    "# Load data from csv\n",
    "housing = pd.read_csv('kc_house_data.csv')\n",
    "\n",
    "# Convert price column to float32\n",
    "price = tf.cast(housing['price'], tf.float32)\n",
    "\n",
    "# Convert waterfront column to boolean\n",
    "waterfront = tf.cast(housing['waterfront'], tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "## Fundamental tensorflow operation\n",
    "- Loss functions are tensors that represent the difference between the model's predictions and the true values.\n",
    "- Used to train a model\n",
    "- Measure of model fit  \n",
    "__Higher value -> worse fit (loss values can be considered as grade of unhappiness)__\n",
    "- Minimize the loss function\n",
    "\n",
    "## Tensorflow has operations for common loss functions\n",
    "- Mean Squared Error (MSE)\n",
    "- Mean Absolute Error (MAE) \n",
    "- Huber Loss\n",
    "\n",
    "![Loss Functions](../../data/img/loss_function_types.png)\n",
    "\n",
    "In blue color, the MSE loss function, in orange color, the MAE loss function and in green color, the Huber loss function.  \n",
    "\n",
    "In can be seen, that MSE strongly penalizes outliers due to high values on the y-axis.\n",
    "\n",
    "The MAE and Huber loss functions are similar, but the Huber loss function is more robust to outliers - because it is a quadratic function and thus does not penalize outliers as much as the MSE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Defining a loss function\n",
    "def linear_regression(intercept, slope,features):\n",
    "    return intercept + slope * features\n",
    "\n",
    "def loss_function(intercept, slope,targets, features):\n",
    "    # Compute the predictions for a linear model\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    \n",
    "    # Return the loss\n",
    "    return tf.keras.losses.mse(targets, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with TensorFlow\n",
    "In this exercise, you will use TensorFlow to fit a linear regression model to the data in the file 'data/boston_house_prices.csv'. A linear regression model is a model that maps an input feature vector to a scalar output value as follows:\n",
    "$$y = m_1 \\cdot x_1 + ... + m_n \\cdot x_n + c $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.099176 1.0991883\n"
     ]
    }
   ],
   "source": [
    "### Example  \n",
    "# Convert price column to float32\n",
    "price = tf.cast(housing['price'], tf.float32)\n",
    "\n",
    "# Convert size column to float32\n",
    "size = tf.cast(housing['sqft_living'], tf.float32)\n",
    "\n",
    "# Define the intercept and slope\n",
    "intercept = tf.Variable(0.1, np.float32)\n",
    "slope = tf.Variable(0.1, np.float32)\n",
    "\n",
    "# Define the linear regression model\n",
    "def linear_regression(intercept, slope,features = size):\n",
    "    return intercept + slope * features\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(intercept, slope, targets = price, features = size):\n",
    "    # Compute the predictions for a linear model\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "\n",
    "# Define the optimizer\n",
    "opt = tf.keras.optimizers.Adam() # the learning rate is 0.1\n",
    "\n",
    "# Minimize the loss function and print loss\n",
    "for j in range(1000):\n",
    "    opt.minimize(\n",
    "        lambda: loss_function(intercept, slope), \n",
    "        var_list=[intercept, slope])\n",
    "    #print(loss_function(intercept, slope))\n",
    "\n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear model\n",
    "In this exercise, we will pick up where the previous exercise ended. The intercept and slope, __intercept__ and __slope__, have been defined and initialized. Additionally, a function has been defined, __loss_function(__intercept__, __slope__)__, which computes the loss using the data and model variables.\n",
    "\n",
    "You will now define an optimization operation as opt. You will then train a univariate linear model by minimizing the loss to find the optimal values of __intercept__ and __slope__. Note that the opt operation will try to move closer to the optimum with each step, but will require many steps to find it. Thus, you must repeatedly execute the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424840500000.0\n",
      "411406070000.0\n",
      "398277700000.0\n",
      "385485440000.0\n",
      "373046500000.0\n",
      "360967600000.0\n",
      "349248780000.0\n",
      "337886400000.0\n",
      "326875200000.0\n",
      "316208840000.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "intercept = tf.Variable(0.1, np.float32)\n",
    "slope = tf.Variable(0.1, np.float32)\n",
    "\n",
    "# Initialize an Adam optimizer\n",
    "opt = keras.optimizers.Adam(0.5)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Apply minimize, pass the loss function, and supply the variables\n",
    "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
    "\n",
    "\t# Print every 10th value of the loss\n",
    "\tif j % 10 == 0:\n",
    "\t\tprint(loss_function(intercept, slope).numpy())\n",
    "\n",
    "# Plot data and regression line\n",
    "#plot_results(intercept, slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "In most cases, performing a univariate linear regression will not yield a model that is useful for making accurate predictions. In this exercise, you will perform a multiple regression, which uses more than one feature.\n",
    "\n",
    "You will use __price_log__ as your target and __size_log__ and __bedrooms__ as your features. Each of these tensors has been defined and is available. You will also switch from using the the mean squared error loss to the mean absolute error loss: __keras.losses.mae()__.\n",
    "\n",
    "Finally, the predicted values are computed as follows: \n",
    "\n",
    "__params[0] + feature1 * params[1] + feature2*params[2]__.\n",
    "\n",
    "Note that we've defined a vector of parameters, params, as a variable, rather than using three variables. Here, params[0] is the intercept and params[1] and params[2] are the slopes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.101 0.101\n",
      "0.10199899 0.10199937\n",
      "0.102998435 0.10299699\n",
      "0.10400031 0.10398846\n",
      "0.10500375 0.104987085\n",
      "0.10600952 0.10599103\n",
      "0.1070124 0.10697429\n",
      "0.10801417 0.10793939\n",
      "0.10901249 0.108877264\n",
      "0.11000538 0.10979223\n",
      "0.11099832 0.110718474\n",
      "0.11199712 0.11165525\n",
      "0.11299419 0.112579174\n",
      "0.113997646 0.11351572\n",
      "0.1150085 0.11445159\n",
      "0.11601862 0.11537831\n",
      "0.11702207 0.11631398\n",
      "0.11802591 0.11729606\n",
      "0.119032614 0.11827049\n",
      "0.12003659 0.11923201\n",
      "0.121043846 0.12017934\n",
      "0.12205039 0.12112183\n",
      "0.12305397 0.12205291\n",
      "0.12405374 0.12298835\n",
      "0.12506042 0.12392123\n",
      "0.12606408 0.124855824\n",
      "0.12706958 0.12578833\n",
      "0.12807287 0.1267062\n",
      "0.12908298 0.12765504\n",
      "0.13009037 0.12864174\n",
      "0.13109654 0.12961163\n",
      "0.13210507 0.13059197\n",
      "0.1331175 0.13155898\n",
      "0.13412856 0.13252482\n",
      "0.13513504 0.13352256\n",
      "0.13613608 0.13451622\n",
      "0.13713042 0.13548161\n",
      "0.13813241 0.1364749\n",
      "0.13913755 0.13747682\n",
      "0.14015149 0.13853979\n",
      "0.1411694 0.13960584\n",
      "0.14218828 0.14065053\n",
      "0.14320695 0.14168672\n",
      "0.14422125 0.14274193\n",
      "0.14524846 0.14385107\n",
      "0.14626478 0.14496326\n",
      "0.14728406 0.14604785\n",
      "0.14829832 0.14708742\n",
      "0.14930189 0.14810307\n",
      "0.15030812 0.14911857\n",
      "0.15131606 0.15015925\n",
      "0.15232053 0.15116704\n",
      "0.15331547 0.15216398\n",
      "0.15430044 0.15313818\n",
      "0.15528905 0.1541102\n",
      "0.15626706 0.15504166\n",
      "0.15725183 0.15595512\n",
      "0.15824522 0.15688694\n",
      "0.15924059 0.15781721\n",
      "0.16023633 0.15873948\n",
      "0.16123593 0.15966392\n",
      "0.16223288 0.1605791\n",
      "0.16322431 0.16149111\n",
      "0.16421154 0.16236775\n",
      "0.16519958 0.16327374\n",
      "0.16619419 0.16422889\n",
      "0.16718403 0.16529246\n",
      "0.16817984 0.16633034\n",
      "0.16916527 0.16732952\n",
      "0.17015137 0.16832109\n",
      "0.17114255 0.16941476\n",
      "0.17213027 0.17045115\n",
      "0.17313051 0.17154424\n",
      "0.17413682 0.1726217\n",
      "0.17514738 0.1736776\n",
      "0.17615207 0.17470045\n",
      "0.17715406 0.17572348\n",
      "0.17815009 0.176765\n",
      "0.17914134 0.17779057\n",
      "0.18013605 0.17877632\n",
      "0.18113425 0.17974262\n",
      "0.18212664 0.18068664\n",
      "0.18310753 0.18160987\n",
      "0.18408413 0.18249735\n",
      "0.1850524 0.18342896\n",
      "0.18601747 0.18436088\n",
      "0.18698479 0.18532397\n",
      "0.18795186 0.18627265\n",
      "0.18891634 0.18719928\n",
      "0.18988168 0.18810005\n",
      "0.19084741 0.18902043\n",
      "0.19181308 0.18991622\n",
      "0.19279332 0.19085975\n",
      "0.19377492 0.19180995\n",
      "0.19475561 0.19276164\n",
      "0.19573383 0.19369337\n",
      "0.19671704 0.19460517\n",
      "0.19769496 0.19553702\n",
      "0.19867282 0.19644184\n",
      "0.19965278 0.19732589\n",
      "0.20062774 0.19817534\n",
      "0.20160002 0.19901788\n",
      "0.20256613 0.19983192\n",
      "0.20353547 0.20064358\n",
      "0.20451441 0.20148756\n",
      "0.20549026 0.2024037\n",
      "0.20646732 0.2033046\n",
      "0.20743772 0.20418848\n",
      "0.20840947 0.20504397\n",
      "0.20938675 0.20594825\n",
      "0.21036626 0.20683667\n",
      "0.21133728 0.2077074\n",
      "0.21232428 0.20858851\n",
      "0.2133057 0.20943686\n",
      "0.21429342 0.21024829\n",
      "0.21528465 0.21105598\n",
      "0.21628231 0.2118916\n",
      "0.21727836 0.21271467\n",
      "0.2182711 0.213542\n",
      "0.21927848 0.21442595\n",
      "0.22028139 0.21528548\n",
      "0.22127981 0.21616289\n",
      "0.22228415 0.21703234\n",
      "0.2232904 0.21792682\n",
      "0.22428863 0.21882038\n",
      "0.22528607 0.21968465\n",
      "0.2262837 0.22052823\n",
      "0.22727714 0.22143686\n",
      "0.22826554 0.2223417\n",
      "0.22923681 0.22323394\n",
      "0.23021162 0.22414932\n",
      "0.23118797 0.2250718\n",
      "0.23217885 0.22605155\n",
      "0.23316053 0.2269873\n",
      "0.2341645 0.22801626\n",
      "0.2351682 0.22903475\n",
      "0.2361632 0.23006263\n",
      "0.23716041 0.23107123\n",
      "0.23815207 0.23207234\n",
      "0.23915122 0.23304991\n",
      "0.24015242 0.23399884\n",
      "0.24115197 0.2349377\n",
      "0.24215677 0.23583554\n",
      "0.24316177 0.23678426\n",
      "0.24415736 0.2376948\n",
      "0.24515548 0.23859282\n",
      "0.2461549 0.23952799\n",
      "0.24714774 0.24044873\n",
      "0.24814387 0.24137048\n",
      "0.24914531 0.24228811\n",
      "0.25014848 0.24317208\n",
      "0.25115687 0.24406344\n",
      "0.25218406 0.24498644\n",
      "0.253205 0.24589358\n",
      "0.25421923 0.246771\n",
      "0.2552354 0.24767816\n",
      "0.2562502 0.24858925\n",
      "0.25726593 0.24950112\n",
      "0.25827697 0.25038087\n",
      "0.25928113 0.251277\n",
      "0.26028508 0.25216067\n",
      "0.2612916 0.25305513\n",
      "0.26230136 0.25392947\n",
      "0.26331764 0.254785\n",
      "0.26433167 0.2556114\n",
      "0.26534095 0.25642186\n",
      "0.26634347 0.25722563\n",
      "0.26734278 0.25804636\n",
      "0.26834595 0.25887725\n",
      "0.26934683 0.25973758\n",
      "0.27034253 0.26058692\n",
      "0.27133116 0.26141796\n",
      "0.27232507 0.26224437\n",
      "0.27332178 0.2630748\n",
      "0.2743152 0.2638956\n",
      "0.27530771 0.264751\n",
      "0.2763013 0.2656129\n",
      "0.27729487 0.26647282\n",
      "0.2782872 0.26733792\n",
      "0.27927524 0.26817244\n",
      "0.28026244 0.26897508\n",
      "0.2812524 0.26979095\n",
      "0.28224748 0.27068138\n",
      "0.28324682 0.2715728\n",
      "0.28425983 0.27245757\n",
      "0.2852727 0.2733322\n",
      "0.28628537 0.274185\n",
      "0.28730482 0.27504504\n",
      "0.2883259 0.27592897\n",
      "0.28934368 0.2767989\n",
      "0.2903614 0.27769524\n",
      "0.29137486 0.278622\n",
      "0.29239437 0.27953503\n",
      "0.29341167 0.28043807\n",
      "0.29442567 0.28133672\n",
      "0.2954346 0.28222957\n",
      "0.29644752 0.28309205\n",
      "0.29746994 0.28390452\n",
      "0.29850298 0.28471455\n",
      "0.29954642 0.28549606\n",
      "0.30060217 0.2862351\n",
      "0.30166686 0.2869702\n",
      "0.30274138 0.28771356\n",
      "0.30382037 0.2884331\n",
      "0.3049107 0.2893411\n",
      "0.30599782 0.29019037\n",
      "0.30707794 0.29099062\n",
      "0.3081494 0.2917492\n",
      "0.30922684 0.29250607\n",
      "0.31029794 0.29327625\n",
      "0.31136203 0.29401442\n",
      "0.31243187 0.2947178\n",
      "0.3134956 0.29538438\n",
      "0.31456718 0.29610583\n",
      "0.3156396 0.29682958\n",
      "0.31673324 0.29759353\n",
      "0.31781912 0.29831016\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow, pandas, and numpy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define trainable variables\n",
    "intercept = tf.Variable(0.1, tf.float32)\n",
    "slope = tf.Variable(0.1, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "    return intercept + features*slope\n",
    "\n",
    "# Compute predicted values and return loss function\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "    predictions = linear_regression(intercept, slope, features)\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "\n",
    "# Define optimization operation\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Load the data in batches from pandas\n",
    "for batch in pd.read_csv('./kc_house_data.csv', chunksize=100):\n",
    "    # Extract the target and feature columns    \n",
    "    price_batch = np.array(batch['price'], np.float32)    \n",
    "    size_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "    \n",
    "    # Minimize the loss function    \n",
    "    opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch),\n",
    "                 var_list=[intercept, slope])\n",
    "    \n",
    "    # Print parameter values\n",
    "    print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Batch and Full-Sample training\n",
    "| Full Sample                           | Batch Training              |\n",
    "| --------------------------------------| --------------------------- |\n",
    "| One update per epoch                  | Multiple updates per epoch  |\n",
    "| Accepts dataset without modification  | Requires divison of dataset |\n",
    "| Limited by memory                     | No limit on dataset size    |   \n",
    "\n",
    "## Preparing to batch train\n",
    "Before we can train a linear model in batches, we must first define variables, a loss function, and an optimization operation. In this exercise, we will prepare to train a model that will predict price_batch, a batch of house prices, using size_batch, a batch of lot sizes in square feet. In contrast to the previous lesson, we will do this by loading batches of data using pandas, converting it to numpy arrays, and then using it to minimize the loss function in steps.\n",
    "\n",
    "Variable(), keras(), and float32 have been imported for you. Note that you should not set default argument values for either the model or loss function, since we will generate the data in batches during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the intercept and slope\n",
    "intercept = tf.Variable(10.0, tf.float32)\n",
    "slope = tf.Variable(0.5, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "\t# Define the predicted values\n",
    "\treturn intercept + slope * features\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(intercept,slope,targets,features):\n",
    "\t# Define the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    " \t# Define the MSE loss\n",
    "\treturn keras.losses.mse(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a linear model in batches\n",
    "In this exercise, we will train a linear regression model in batches, starting where we left off in the previous exercise. We will do this by stepping through the dataset in batches and updating the model's variables, __intercept__ and __slope__, after each step. This approach will allow us to train with datasets that are otherwise too large to hold in memory.\n",
    "\n",
    "Note that the loss function, __loss_function(intercept, slope, targets, features)__, has been defined for you. Additionally, __keras__ has been imported for you and __numpy__ is available as np. The trainable variables should be entered into var_list in the order in which they appear as loss function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.217888 0.7016\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
    "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "\n",
    "\t# Extract the price values for the current batch\n",
    "\tprice_batch = np.array(batch['price'], np.float32)\n",
    "\n",
    "\t# Complete the loss, fill in the variable list, and minimize\n",
    "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "# Print trained parameters\n",
    "print(intercept.numpy(), slope.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layers\n",
    "In this chapter, we will focus on training neural networks in TensorFlow. We will start with an overview of a frequently used component of neural networks: the dense layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Low level API - Linear Algebra Way\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define inputs (features) \n",
    "inputs = tf.constant([[1.0, 35.0]])\n",
    "\n",
    "# Define weights\n",
    "weights = tf.Variable([[-0.05], [-0.01]])\n",
    "\n",
    "# Define bias\n",
    "bias = tf.Variable([0.5])\n",
    "\n",
    "# Multiply inputs (features) by the weights\n",
    "product = tf.matmul(inputs, weights)\n",
    "\n",
    "# Define dense layer\n",
    "dense = tf.keras.activations.sigmoid(product + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### High level API - High Level API Way\n",
    "\n",
    "# Define input features layer\n",
    "inputs = tf.constant([[1.0, 35.0]])\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(units=10, \n",
    "                               activation='sigmoid', \n",
    "                               )(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(units=5, \n",
    "                               activation='sigmoid', \n",
    "                               )(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(units=1, \n",
    "                               activation='sigmoid', \n",
    "                               )(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| High Level Approach | Low Level Approach |\n",
    "| --------------------| ------------------ |\n",
    "```dense = keras.layers.Dense(units=10,activation='sigmoid') ``` | ```prod= matmul(inputs,weights) & dense = keras.avtivations.sigmoid(prod)```|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The linear algebra of dense layers\n",
    "There are two ways to define a dense layer in tensorflow. \n",
    "\n",
    "The first involves the use of low-level, linear algebraic operations. \n",
    "\n",
    "The second makes use of high-level keras operations. In this exercise, we will use the first method to construct the network shown in the image below.\n",
    "\n",
    "![NN](../../data/img/3_2_1_network2.png)\n",
    "\n",
    "This image depicts an neural network with 5 input nodes and 3 output nodes.\n",
    "The input layer contains 3 features -- education, marital status, and age -- which are available as borrower_features. The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "\n",
    "For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function. Note that Variable(), ones(), matmul(), and keras() have been imported from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dense1's output shape: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras, Variable, constant, matmul # instead of prefixing this stuff by tf.*\n",
    "\n",
    "# Initialize borrower_features\n",
    "borrower_features = tf.constant([\n",
    "        [3.,3.,23.],\n",
    "        [2.,1.,24.],\n",
    "        [1.,1.,49.],\n",
    "        [1.,1.,49.],\n",
    "        [2.,1.,29.],\n",
    "    ],dtype=tf.float32)\n",
    "\n",
    "# Initialize bias1\n",
    "bias1 = Variable(1.0)\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = Variable(\n",
    "    ones((3, 2))\n",
    "    )\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = tf.matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"\\n dense1's output shape: {}\".format(dense1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " prediction: 0.9525741338729858\n",
      "\n",
      " actual: 1\n"
     ]
    }
   ],
   "source": [
    "# From previous step\n",
    "bias1 = Variable(1.0)\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Initialize bias2 and weights2\n",
    "bias2 = Variable(1.0)\n",
    "weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = matmul(dense1,weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = keras.activations.sigmoid(product2 + bias2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The low-level approach with multiple examples\n",
    "In this exercise, we'll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We'll assume the model is trained and the first layer weights, weights1, and bias, bias1, are available. We'll then perform matrix multiplication of the borrower_features tensor by the weights1 variable. Recall that the borrower_features tensor includes education, marital status, and age. Finally, we'll apply the sigmoid function to the elements of products1 + bias1, yielding dense1.\n",
    "\n",
    "$$ products1 = \\begin{bmatrix} 3 & 3 & 23 \\\\ 2 & 1 & 24 \\\\ 1 & 1 & 49 \\\\ 1 & 1 & 49 \\\\ 2 & 1 & 29 \\end{bmatrix} \\begin{bmatrix} -0.6 & 0.6 \\\\ 0.8 & -0.3 \\\\ -0.09 & -0.08 \\end{bmatrix}$$ \n",
    "\n",
    "Note that matmul() and keras() have been imported from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of borrower_features: (r,c) (5, 3)\n",
      "\n",
      " shape of weights1: (r,c) (3, 2)\n",
      "\n",
      " shape of bias1: (r,c) ()\n",
      "\n",
      " shape of dense1: (r,c) (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compute the product of borrower_features and weights1\n",
    "products1 = matmul(borrower_features,weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = keras.activations.sigmoid(products1+bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: (r,c)', borrower_features.shape)\n",
    "print('\\n shape of weights1: (r,c)', weights1.shape)\n",
    "print('\\n shape of bias1: (r,c)', bias1.shape)\n",
    "print('\\n shape of dense1: (r,c)', dense1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the dense layer operation\n",
    "We've now seen how to define dense layers in tensorflow using linear algebra. In this exercise, we'll skip the linear algebra and let keras work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "\n",
    "![NN](../../data/img/10_7_3_1_network.png)\n",
    "\n",
    "To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100x10 tensor: borrower_features. Additionally, the keras.layers module is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = tf.constant(\n",
    "        [\n",
    "        [6.96469188e-01, 2.86139339e-01, 2.26851448e-01, 5.51314771e-01,\n",
    "        7.19468951e-01, 4.23106462e-01, 9.80764210e-01, 6.84829712e-01,\n",
    "        4.80931908e-01, 3.92117530e-01],\n",
    "       [3.43178004e-01, 7.29049683e-01, 4.38572258e-01, 5.96778952e-02,\n",
    "        3.98044258e-01, 7.37995386e-01, 1.82491735e-01, 1.75451756e-01,\n",
    "        5.31551361e-01, 5.31827569e-01],\n",
    "       [6.34400964e-01, 8.49431813e-01, 7.24455297e-01, 6.11023486e-01,\n",
    "        7.22443402e-01, 3.22958916e-01, 3.61788660e-01, 2.28263229e-01,\n",
    "        2.93714046e-01, 6.30976140e-01],\n",
    "       [9.21049416e-02, 4.33701187e-01, 4.30862755e-01, 4.93685097e-01,\n",
    "        4.25830305e-01, 3.12261224e-01, 4.26351309e-01, 8.93389165e-01,\n",
    "        9.44160044e-01, 5.01836658e-01],\n",
    "       [6.23952925e-01, 1.15618393e-01, 3.17285478e-01, 4.14826214e-01,\n",
    "        8.66309166e-01, 2.50455379e-01, 4.83034253e-01, 9.85559762e-01,\n",
    "        5.19485116e-01, 6.12894535e-01],\n",
    "       [1.20628662e-01, 8.26340795e-01, 6.03060126e-01, 5.45068026e-01,\n",
    "        3.42763841e-01, 3.04120779e-01, 4.17022198e-01, 6.81300759e-01,\n",
    "        8.75456870e-01, 5.10422349e-01],\n",
    "       [6.69313788e-01, 5.85936546e-01, 6.24903500e-01, 6.74689054e-01,\n",
    "        8.42342436e-01, 8.31949860e-02, 7.63682842e-01, 2.43666381e-01,\n",
    "        1.94222957e-01, 5.72456956e-01],\n",
    "       [9.57125202e-02, 8.85326803e-01, 6.27248943e-01, 7.23416328e-01,\n",
    "        1.61292069e-02, 5.94431877e-01, 5.56785166e-01, 1.58959642e-01,\n",
    "        1.53070509e-01, 6.95529521e-01],\n",
    "       [3.18766415e-01, 6.91970289e-01, 5.54383278e-01, 3.88950586e-01,\n",
    "        9.25132513e-01, 8.41669977e-01, 3.57397556e-01, 4.35914621e-02,\n",
    "        3.04768085e-01, 3.98185670e-01],\n",
    "       [7.04958856e-01, 9.95358467e-01, 3.55914861e-01, 7.62547791e-01,\n",
    "        5.93176901e-01, 6.91701770e-01, 1.51127458e-01, 3.98876280e-01,\n",
    "        2.40855902e-01, 3.43456000e-01],\n",
    "       [5.13128161e-01, 6.66624546e-01, 1.05908483e-01, 1.30894944e-01,\n",
    "        3.21980596e-01, 6.61564350e-01, 8.46506238e-01, 5.53257346e-01,\n",
    "        8.54452491e-01, 3.84837806e-01],\n",
    "       [3.16787899e-01, 3.54264677e-01, 1.71081826e-01, 8.29112649e-01,\n",
    "        3.38670850e-01, 5.52370071e-01, 5.78551471e-01, 5.21533072e-01,\n",
    "        2.68806447e-03, 9.88345444e-01],\n",
    "       [9.05341566e-01, 2.07635865e-01, 2.92489409e-01, 5.20010173e-01,\n",
    "        9.01911378e-01, 9.83630896e-01, 2.57542074e-01, 5.64359069e-01,\n",
    "        8.06968689e-01, 3.94370049e-01],\n",
    "       [7.31073022e-01, 1.61069021e-01, 6.00698590e-01, 8.65864456e-01,\n",
    "        9.83521581e-01, 7.93657899e-02, 4.28347290e-01, 2.04542860e-01,\n",
    "        4.50636476e-01, 5.47763586e-01],\n",
    "       [9.33267102e-02, 2.96860784e-01, 9.27584231e-01, 5.69003761e-01,\n",
    "        4.57412004e-01, 7.53525972e-01, 7.41862178e-01, 4.85790335e-02,\n",
    "        7.08697379e-01, 8.39243352e-01],\n",
    "       [1.65937886e-01, 7.80997932e-01, 2.86536604e-01, 3.06469738e-01,\n",
    "        6.65261447e-01, 1.11392170e-01, 6.64872468e-01, 8.87856781e-01,\n",
    "        6.96311295e-01, 4.40327883e-01],\n",
    "       [4.38214391e-01, 7.65096068e-01, 5.65641999e-01, 8.49041641e-02,\n",
    "        5.82671106e-01, 8.14843714e-01, 3.37066382e-01, 9.27576602e-01,\n",
    "        7.50716984e-01, 5.74063838e-01],\n",
    "       [7.51644015e-01, 7.91489631e-02, 8.59389067e-01, 8.21504116e-01,\n",
    "        9.09871638e-01, 1.28631204e-01, 8.17800835e-02, 1.38415575e-01,\n",
    "        3.99378717e-01, 4.24306870e-01],\n",
    "       [5.62218368e-01, 1.22243546e-01, 2.01399505e-01, 8.11644375e-01,\n",
    "        4.67987567e-01, 8.07938218e-01, 7.42637832e-03, 5.51592708e-01,\n",
    "        9.31932151e-01, 5.82175434e-01],\n",
    "       [2.06095725e-01, 7.17757583e-01, 3.78985852e-01, 6.68383956e-01,\n",
    "        2.93197222e-02, 6.35900378e-01, 3.21979336e-02, 7.44780660e-01,\n",
    "        4.72912997e-01, 1.21754356e-01],\n",
    "       [5.42635918e-01, 6.67744428e-02, 6.53364897e-01, 9.96086299e-01,\n",
    "        7.69397318e-01, 5.73774099e-01, 1.02635257e-01, 6.99834049e-01,\n",
    "        6.61167860e-01, 4.90971319e-02],\n",
    "       [7.92299330e-01, 5.18716574e-01, 4.25867707e-01, 7.88187146e-01,\n",
    "        4.11569238e-01, 4.81026262e-01, 1.81628838e-01, 3.21318895e-01,\n",
    "        8.45533013e-01, 1.86903745e-01],\n",
    "       [4.17291075e-01, 9.89034534e-01, 2.36599818e-01, 9.16832328e-01,\n",
    "        9.18397486e-01, 9.12963450e-02, 4.63652730e-01, 5.02216339e-01,\n",
    "        3.13668936e-01, 4.73395362e-02],\n",
    "       [2.41685644e-01, 9.55296382e-02, 2.38249913e-01, 8.07791114e-01,\n",
    "        8.94978285e-01, 4.32228930e-02, 3.01946849e-01, 9.80582178e-01,\n",
    "        5.39504826e-01, 6.26309335e-01],\n",
    "       [5.54540846e-03, 4.84909445e-01, 9.88328516e-01, 3.75185519e-01,\n",
    "        9.70381573e-02, 4.61908758e-01, 9.63004470e-01, 3.41830611e-01,\n",
    "        7.98922718e-01, 7.98846304e-01],\n",
    "       [2.08248302e-01, 4.43367690e-01, 7.15601265e-01, 4.10519779e-01,\n",
    "        1.91006958e-01, 9.67494309e-01, 6.50750339e-01, 8.65459859e-01,\n",
    "        2.52423584e-02, 2.66905814e-01],\n",
    "       [5.02071083e-01, 6.74486384e-02, 9.93033290e-01, 2.36462399e-01,\n",
    "        3.74292195e-01, 2.14011908e-01, 1.05445869e-01, 2.32479781e-01,\n",
    "        3.00610125e-01, 6.34442270e-01],\n",
    "       [2.81234771e-01, 3.62276763e-01, 5.94284385e-03, 3.65719140e-01,\n",
    "        5.33885956e-01, 1.62015840e-01, 5.97433090e-01, 2.93152481e-01,\n",
    "        6.32050514e-01, 2.61966046e-02],\n",
    "       [8.87593448e-01, 1.61186308e-02, 1.26958027e-01, 7.77162433e-01,\n",
    "        4.58952338e-02, 7.10998714e-01, 9.71046150e-01, 8.71682942e-01,\n",
    "        7.10161626e-01, 9.58509743e-01],\n",
    "       [4.29813325e-01, 8.72878909e-01, 3.55957657e-01, 9.29763675e-01,\n",
    "        1.48777649e-01, 9.40029025e-01, 8.32716227e-01, 8.46054852e-01,\n",
    "        1.23923011e-01, 5.96486926e-01],\n",
    "       [1.63924806e-02, 7.21184373e-01, 7.73751410e-03, 8.48222747e-02,\n",
    "        2.25498408e-01, 8.75124514e-01, 3.63576323e-01, 5.39959908e-01,\n",
    "        5.68103194e-01, 2.25463361e-01],\n",
    "       [5.72146773e-01, 6.60951793e-01, 2.98245400e-01, 4.18626845e-01,\n",
    "        4.53088939e-01, 9.32350636e-01, 5.87493777e-01, 9.48252380e-01,\n",
    "        5.56034744e-01, 5.00561416e-01],\n",
    "       [3.53221106e-03, 4.80889052e-01, 9.27455008e-01, 1.98365688e-01,\n",
    "        5.20911328e-02, 4.06778902e-01, 3.72396469e-01, 8.57153058e-01,\n",
    "        2.66111158e-02, 9.20149207e-01],\n",
    "       [6.80903018e-01, 9.04226005e-01, 6.07529044e-01, 8.11953306e-01,\n",
    "        3.35543871e-01, 3.49566221e-01, 3.89874220e-01, 7.54797101e-01,\n",
    "        3.69291186e-01, 2.42219806e-01],\n",
    "       [9.37668383e-01, 9.08011079e-01, 3.48797321e-01, 6.34638071e-01,\n",
    "        2.73842216e-01, 2.06115127e-01, 3.36339533e-01, 3.27099890e-01,\n",
    "        8.82276118e-01, 8.22303832e-01],\n",
    "       [7.09623218e-01, 9.59345222e-01, 4.22543347e-01, 2.45033041e-01,\n",
    "        1.17398441e-01, 3.01053345e-01, 1.45263731e-01, 9.21861008e-02,\n",
    "        6.02932215e-01, 3.64187449e-01],\n",
    "       [5.64570367e-01, 1.91335723e-01, 6.76905870e-01, 2.15505451e-01,\n",
    "        2.78023601e-01, 7.41760433e-01, 5.59737921e-01, 3.34836423e-01,\n",
    "        5.42988777e-01, 6.93984687e-01],\n",
    "       [9.12132144e-01, 5.80713212e-01, 2.32686386e-01, 7.46697605e-01,\n",
    "        7.77769029e-01, 2.00401321e-01, 8.20574224e-01, 4.64934856e-01,\n",
    "        7.79766679e-01, 2.37478226e-01],\n",
    "       [3.32580268e-01, 9.53697145e-01, 6.57815099e-01, 7.72877812e-01,\n",
    "        6.88374341e-01, 2.04304114e-01, 4.70688760e-01, 8.08963895e-01,\n",
    "        6.75035119e-01, 6.02788571e-03],\n",
    "       [8.74077454e-02, 3.46794724e-01, 9.44365561e-01, 4.91190493e-01,\n",
    "        2.70176262e-01, 3.60423714e-01, 2.10652635e-01, 4.21200067e-01,\n",
    "        2.18035445e-01, 8.45752478e-01],\n",
    "       [4.56270605e-01, 2.79802024e-01, 9.32891667e-01, 3.14351350e-01,\n",
    "        9.09714639e-01, 4.34180908e-02, 7.07115054e-01, 4.83889043e-01,\n",
    "        4.44221050e-01, 3.63233462e-02],\n",
    "       [4.06831913e-02, 3.32753628e-01, 9.47119534e-01, 6.17659986e-01,\n",
    "        3.68874848e-01, 6.11977041e-01, 2.06131533e-01, 1.65066436e-01,\n",
    "        3.61817271e-01, 8.63353372e-01],\n",
    "       [5.09401739e-01, 2.96901524e-01, 9.50251639e-01, 8.15966070e-01,\n",
    "        3.22973937e-01, 9.72098231e-01, 9.87351120e-01, 4.08660144e-01,\n",
    "        6.55923128e-01, 4.05653208e-01],\n",
    "       [2.57348120e-01, 8.26526731e-02, 2.63610333e-01, 2.71479845e-01,\n",
    "        3.98639083e-01, 1.84886038e-01, 9.53818381e-01, 1.02879882e-01,\n",
    "        6.25208557e-01, 4.41697389e-01],\n",
    "       [4.23518062e-01, 3.71991783e-01, 8.68314683e-01, 2.80476987e-01,\n",
    "        2.05761567e-02, 9.18097019e-01, 8.64480257e-01, 2.76901782e-01,\n",
    "        5.23487568e-01, 1.09088197e-01],\n",
    "       [9.34270695e-02, 8.37466121e-01, 4.10265714e-01, 6.61716521e-01,\n",
    "        9.43200588e-01, 2.45130599e-01, 1.31598311e-02, 2.41484065e-02,\n",
    "        7.09385693e-01, 9.24551904e-01],\n",
    "       [4.67330277e-01, 3.75109136e-01, 5.42860448e-01, 8.58916819e-01,\n",
    "        6.52153850e-01, 2.32979894e-01, 7.74580181e-01, 1.34613499e-01,\n",
    "        1.65559977e-01, 6.12682283e-01],\n",
    "       [2.38783404e-01, 7.04778552e-01, 3.49518538e-01, 2.77423948e-01,\n",
    "        9.98918414e-01, 4.06161249e-02, 6.45822525e-01, 3.86995859e-02,\n",
    "        7.60210276e-01, 2.30089962e-01],\n",
    "       [8.98318663e-02, 6.48449719e-01, 7.32601225e-01, 6.78095341e-01,\n",
    "        5.19009456e-02, 2.94306934e-01, 4.51088339e-01, 2.87103295e-01,\n",
    "        8.10513437e-01, 1.31115109e-01],\n",
    "       [6.12179339e-01, 9.88214970e-01, 9.02556539e-01, 2.22157061e-01,\n",
    "        8.18876142e-05, 9.80597317e-01, 8.82712960e-01, 9.19472456e-01,\n",
    "        4.15503561e-01, 7.44615436e-01],\n",
    "       [2.12831497e-01, 3.92304063e-01, 8.51548076e-01, 1.27612218e-01,\n",
    "        8.93865347e-01, 4.96507972e-01, 4.26095665e-01, 3.05646390e-01,\n",
    "        9.16848779e-01, 5.17623484e-01],\n",
    "       [8.04026365e-01, 8.57651770e-01, 9.22382355e-01, 3.03380728e-01,\n",
    "        3.39810848e-01, 5.95073879e-01, 4.41324145e-01, 9.32842553e-01,\n",
    "        3.97564054e-01, 4.77778047e-01],\n",
    "       [6.17186069e-01, 4.04739499e-01, 9.92478430e-01, 9.88512859e-02,\n",
    "        2.20603317e-01, 3.22655141e-01, 1.47722840e-01, 2.84219235e-01,\n",
    "        7.79245317e-01, 5.22891998e-01],\n",
    "       [3.39536369e-02, 9.82622564e-01, 6.16006494e-01, 5.89394793e-02,\n",
    "        6.61168754e-01, 3.78369361e-01, 1.35673299e-01, 5.63664615e-01,\n",
    "        7.27079928e-01, 6.71126604e-01],\n",
    "       [2.47513160e-01, 5.24866223e-01, 5.37663460e-01, 7.16803372e-01,\n",
    "        3.59867334e-01, 7.97732592e-01, 6.27921820e-01, 3.83316055e-02,\n",
    "        5.46479046e-01, 8.61912072e-01],\n",
    "       [5.67574143e-01, 1.75828263e-01, 5.10376394e-01, 7.56945848e-01,\n",
    "        1.10105194e-01, 8.17099094e-01, 1.67481646e-01, 5.34076512e-01,\n",
    "        3.85743469e-01, 2.48623773e-01],\n",
    "       [6.47432506e-01, 3.73921096e-02, 7.60045826e-01, 5.26940644e-01,\n",
    "        8.75771224e-01, 5.20718336e-01, 3.50331701e-02, 1.43600971e-01,\n",
    "        7.95604587e-01, 4.91976053e-01],\n",
    "       [4.41879272e-01, 3.18434775e-01, 2.84549206e-01, 9.65886295e-01,\n",
    "        4.32969332e-01, 8.84003043e-01, 6.48163140e-01, 8.58427644e-01,\n",
    "        8.52449536e-01, 9.56312001e-01],\n",
    "       [6.97942257e-01, 8.05396914e-01, 7.33127892e-01, 6.05226815e-01,\n",
    "        7.17354119e-01, 7.15750396e-01, 4.09077927e-02, 5.16110837e-01,\n",
    "        7.92651355e-01, 2.42962182e-01],\n",
    "       [4.65147972e-01, 4.34985697e-01, 4.02787179e-01, 1.21839531e-01,\n",
    "        5.25711536e-01, 4.46248353e-01, 6.63392782e-01, 5.49413085e-01,\n",
    "        2.75429301e-02, 3.19179893e-02],\n",
    "       [7.01359808e-01, 7.07581103e-01, 9.59939122e-01, 8.76704693e-01,\n",
    "        4.68059659e-01, 6.25906527e-01, 4.57181722e-01, 2.22946241e-01,\n",
    "        3.76677006e-01, 1.03884235e-01],\n",
    "       [6.66527092e-01, 1.92030147e-01, 4.75467801e-01, 9.67436612e-01,\n",
    "        3.16689312e-02, 1.51729956e-01, 2.98579186e-01, 9.41806972e-01,\n",
    "        9.08841789e-01, 1.62000835e-01],\n",
    "       [9.81117785e-01, 7.50747502e-01, 5.39977074e-01, 9.31702912e-01,\n",
    "        8.80607128e-01, 3.91316503e-01, 6.56343222e-01, 6.47385120e-01,\n",
    "        3.26968193e-01, 1.79390177e-01],\n",
    "       [4.66809869e-01, 2.63281047e-01, 3.55065137e-01, 9.54143941e-01,\n",
    "        4.61137861e-01, 6.84891462e-01, 3.36229891e-01, 9.95861053e-01,\n",
    "        6.58767581e-01, 1.96009472e-01],\n",
    "       [9.81839970e-02, 9.43180561e-01, 9.44777846e-01, 6.21328354e-01,\n",
    "        1.69914998e-02, 2.25534886e-01, 8.01276803e-01, 8.75459850e-01,\n",
    "        4.53989804e-01, 3.65520626e-01],\n",
    "       [2.74224997e-01, 1.16970517e-01, 1.15744539e-01, 9.52602684e-01,\n",
    "        8.08626115e-01, 1.64779365e-01, 2.07050055e-01, 6.55551553e-01,\n",
    "        7.64664233e-01, 8.10314834e-01],\n",
    "       [1.63337693e-01, 9.84128296e-01, 2.27802068e-01, 5.89415431e-01,\n",
    "        5.87615728e-01, 9.67361867e-01, 6.57667458e-01, 5.84904253e-01,\n",
    "        5.18772602e-01, 7.64657557e-01],\n",
    "       [1.06055260e-01, 2.09190114e-03, 9.52488840e-01, 4.98657674e-01,\n",
    "        3.28335375e-01, 3.68053257e-01, 8.03843319e-01, 3.82370204e-01,\n",
    "        7.70169199e-01, 4.40461993e-01],\n",
    "       [8.44077468e-01, 7.62040615e-02, 4.81128335e-01, 4.66849715e-01,\n",
    "        2.64327973e-01, 9.43614721e-01, 9.05028462e-01, 4.43596303e-01,\n",
    "        9.71596092e-02, 2.06783146e-01],\n",
    "       [2.71491826e-01, 4.84219760e-01, 3.38377118e-01, 7.74136066e-01,\n",
    "        4.76026595e-01, 8.70370507e-01, 9.95781779e-01, 2.19835952e-01,\n",
    "        6.11671388e-01, 8.47502291e-01],\n",
    "       [9.45236623e-01, 2.90086418e-01, 7.27042735e-01, 1.50161488e-02,\n",
    "        8.79142463e-01, 6.39385507e-02, 7.33395398e-01, 9.94610369e-01,\n",
    "        5.01189768e-01, 2.09333986e-01],\n",
    "       [5.94643593e-01, 6.24149978e-01, 6.68072760e-01, 1.72611743e-01,\n",
    "        8.98712695e-01, 6.20991349e-01, 4.35687043e-02, 6.84041083e-01,\n",
    "        1.96084052e-01, 2.73407809e-02],\n",
    "       [5.50953269e-01, 8.13313663e-01, 8.59941125e-01, 1.03520922e-01,\n",
    "        6.63042784e-01, 7.10075200e-01, 2.94516981e-01, 9.71364021e-01,\n",
    "        2.78687477e-01, 6.99821860e-02],\n",
    "       [5.19280374e-01, 6.94314897e-01, 2.44659781e-01, 3.38582188e-01,\n",
    "        5.63627958e-01, 8.86678159e-01, 7.47325897e-01, 2.09591955e-01,\n",
    "        2.51777083e-01, 5.23880661e-01],\n",
    "       [7.68958688e-01, 6.18761778e-01, 5.01324296e-01, 5.97125351e-01,\n",
    "        7.56060004e-01, 5.37079811e-01, 8.97752762e-01, 9.47067499e-01,\n",
    "        9.15354490e-01, 7.54518330e-01],\n",
    "       [2.46321008e-01, 3.85271460e-01, 2.79999942e-01, 6.57660246e-01,\n",
    "        3.24221611e-01, 7.54391611e-01, 1.13509081e-01, 7.75364757e-01,\n",
    "        5.85901976e-01, 8.35388660e-01],\n",
    "       [4.30875659e-01, 6.24964476e-01, 5.54412127e-01, 9.75671291e-01,\n",
    "        7.55474389e-01, 5.44813275e-01, 1.74032092e-01, 9.04114246e-01,\n",
    "        2.05837786e-01, 6.50043249e-01],\n",
    "       [9.36471879e-01, 2.23579630e-01, 2.25923538e-01, 8.51818919e-01,\n",
    "        8.27655017e-01, 3.51703346e-01, 2.65096277e-01, 1.27388477e-01,\n",
    "        9.87936080e-01, 8.35343122e-01],\n",
    "       [8.99391592e-01, 5.13679326e-01, 1.14384830e-01, 5.25803380e-02,\n",
    "        3.30582112e-01, 9.20330405e-01, 9.47581828e-01, 8.41163874e-01,\n",
    "        1.58679143e-01, 4.19923156e-01],\n",
    "       [2.46242926e-01, 2.05349773e-01, 6.84825838e-01, 4.86111671e-01,\n",
    "        3.24909657e-01, 1.00214459e-01, 5.44763386e-01, 3.47025156e-01,\n",
    "        3.91095817e-01, 3.10508728e-01],\n",
    "       [3.87195200e-01, 5.55859566e-01, 1.41438060e-02, 8.47647011e-01,\n",
    "        9.21919882e-01, 5.50529718e-01, 2.68021107e-01, 9.90239024e-01,\n",
    "        3.83194029e-01, 6.93655372e-01],\n",
    "       [6.89952552e-01, 4.34309065e-01, 1.99158162e-01, 9.66579378e-01,\n",
    "        6.36908561e-02, 4.85149384e-01, 2.20730707e-01, 2.93974131e-01,\n",
    "        8.28527331e-01, 3.67265552e-01],\n",
    "       [8.33482668e-02, 1.96309000e-01, 8.60373437e-01, 9.77028847e-01,\n",
    "        2.67982155e-01, 6.75408959e-01, 8.11989978e-02, 7.23465621e-01,\n",
    "        4.16436613e-01, 9.18159902e-01],\n",
    "       [3.11536163e-01, 9.41466987e-01, 5.03247440e-01, 3.48892927e-01,\n",
    "        6.47019625e-01, 2.49746203e-01, 2.29763597e-01, 1.96346447e-01,\n",
    "        9.59899545e-01, 4.92913723e-01],\n",
    "       [7.51614988e-01, 4.73991871e-01, 5.87540150e-01, 5.84138989e-01,\n",
    "        9.79886293e-01, 6.68433130e-01, 2.39769474e-01, 1.51976589e-02,\n",
    "        2.18682140e-01, 4.55519646e-01],\n",
    "       [3.93420339e-01, 8.12326252e-01, 7.85556734e-01, 8.90959650e-02,\n",
    "        9.52010751e-01, 5.27456701e-01, 5.96403956e-01, 4.05056775e-01,\n",
    "        6.49500966e-01, 8.71326327e-01],\n",
    "       [6.73935950e-01, 9.70098555e-01, 7.01122224e-01, 8.21720719e-01,\n",
    "        4.50395830e-02, 6.72698498e-01, 6.54752672e-01, 1.01746053e-01,\n",
    "        8.42387497e-01, 6.14172399e-01],\n",
    "       [9.83280912e-02, 5.94467103e-01, 4.78415847e-01, 2.33293563e-01,\n",
    "        1.97560899e-02, 3.65567267e-01, 6.19851053e-01, 3.29279125e-01,\n",
    "        3.07254642e-01, 7.51121223e-01],\n",
    "       [7.58624673e-01, 7.18765855e-01, 1.01181954e-01, 5.16165972e-01,\n",
    "        5.57798684e-01, 7.44804502e-01, 9.03177738e-01, 3.69038880e-01,\n",
    "        4.28663462e-01, 7.32767463e-01],\n",
    "       [6.62636399e-01, 5.57869911e-01, 3.50139618e-01, 1.95352346e-01,\n",
    "        1.83807373e-01, 8.15832913e-02, 8.12008530e-02, 8.45798194e-01,\n",
    "        3.83672744e-01, 6.07396215e-02],\n",
    "       [8.96425664e-01, 2.23270476e-01, 2.68124431e-01, 1.94497839e-01,\n",
    "        9.67501044e-01, 1.12540089e-01, 7.22163260e-01, 9.32088733e-01,\n",
    "        6.68001294e-01, 8.58726621e-01],\n",
    "       [2.42447108e-01, 6.73927963e-01, 7.00871348e-01, 4.58332509e-01,\n",
    "        8.70545626e-01, 6.94386125e-01, 8.94877791e-01, 7.53204346e-01,\n",
    "        5.20290434e-01, 4.98688221e-01],\n",
    "       [4.53727633e-01, 2.16468628e-02, 5.35141408e-01, 4.22973245e-01,\n",
    "        1.57533601e-01, 1.19069695e-01, 4.49351877e-01, 3.99130546e-02,\n",
    "        9.86579895e-01, 3.78120929e-01],\n",
    "       [3.82109195e-01, 5.11263013e-02, 4.26672339e-01, 1.57454368e-02,\n",
    "        3.00936326e-02, 3.39099228e-01, 8.20968926e-01, 4.58821088e-01,\n",
    "        1.48405796e-02, 1.63220033e-01],\n",
    "       [7.39922702e-01, 7.38293707e-01, 7.54522920e-01, 3.51669371e-01,\n",
    "        3.52276951e-01, 8.02075684e-01, 3.98137897e-01, 7.27191031e-01,\n",
    "        5.81122994e-01, 3.64341676e-01],\n",
    "       [8.00065175e-02, 1.16125375e-01, 8.89558733e-01, 4.52340513e-01,\n",
    "        9.94004548e-01, 3.63896936e-01, 2.49954298e-01, 3.50539327e-01,\n",
    "        3.43086094e-01, 6.37356758e-01],\n",
    "       [1.27375638e-02, 7.63268650e-01, 4.16414618e-01, 4.32239205e-01,\n",
    "        4.81115013e-01, 4.49212462e-01, 4.97470886e-01, 3.45904320e-01,\n",
    "        4.53346133e-01, 4.04651344e-01],\n",
    "       [5.18242717e-01, 6.23269081e-01, 2.41040602e-01, 5.08437157e-01,\n",
    "        5.94621897e-01, 1.69483144e-02, 5.20493746e-01, 2.39293247e-01,\n",
    "        4.04538542e-01, 8.26530159e-01],\n",
    "       [3.26235592e-01, 4.83216912e-01, 2.47411542e-02, 3.08750868e-01,\n",
    "        6.39721096e-01, 3.15161765e-01, 2.05797508e-01, 2.90655673e-01,\n",
    "        9.54378307e-01, 8.68018195e-02],\n",
    "       [4.63357776e-01, 5.83869033e-02, 5.38658261e-01, 1.46035731e-01,\n",
    "        6.34084821e-01, 2.64397472e-01, 6.90915406e-01, 3.47146064e-01,\n",
    "        4.16848855e-03, 2.94894695e-01]\n",
    "       ], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of dense1:  (100, 7)\n",
      "\n",
      " shape of dense2:  (100, 3)\n",
      "\n",
      " shape of predictions:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = keras.layers.Dense(units=7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = keras.layers.Dense(units=3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = keras.layers.Dense(units=1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "- Components of typical hidden layer:\n",
    "    - Linear: Matrix multiplicaiton\n",
    "    - Nonlinear: Activation function\n",
    "\n",
    "Activation functions are mathematical functions attached to every neuron in the ANN. The function calculates a weighted sum of inputs and further adds bias to it and then determines whether the neuron should be activated or not. In simpler words, only those neurons with relevant information for the model’s predictions will be fired or activated in each layer.\n",
    "\n",
    "- Activation functions also help normalize each neuron’s output to arrange between 1 & 0 or between -1 and +1.\n",
    "- The main purpose of the activation function is to introduce non-linearity to the output of a neuron.\n",
    "\n",
    "![](../../data/img/activationFunctions.png)\n",
    "\n",
    "The network you see below is an artificial neural network made of interconnected neurons in different layers. Each neuron is characterized by its weight, bias and activation function.\n",
    "\n",
    "![](../../data/img/NN_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification problems\n",
    "In this exercise, you will again make use of credit card data. The target variable, default, indicates whether a credit card holder defaults on his or her payment in the following period. Since there are only two options--default or not--this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, outputs, and compare those the target variable, default.\n",
    "\n",
    "The tensor of features has been loaded and is available as bill_amounts. Additionally, the constant(), float32, and keras.layers.Dense() operations are available.\n",
    "\n",
    "```python\n",
    "# Construct input layer from features\n",
    "inputs = constant(bill_amounts,dtype=float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(2,activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification problems\n",
    "In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns.\n",
    "\n",
    "As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model's predictions, which are probabilities assigned to the classes. The tensor of features has been loaded and is available as borrower_features. Additionally, the constant(), float32, and keras.layers.Dense() operations are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05630838 0.30366883 0.11778697 0.206271   0.07820393 0.23776093]\n",
      " [0.06690857 0.28333613 0.11430769 0.2337648  0.08415765 0.21752524]\n",
      " [0.0628496  0.2892128  0.11521548 0.22657083 0.07830612 0.22784522]\n",
      " [0.05585167 0.31216508 0.11301144 0.20561698 0.0716241  0.24173072]\n",
      " [0.05212187 0.31581506 0.11419148 0.20424385 0.06761206 0.24601565]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import float32\n",
    "\n",
    "# Construct input layer from borrower features\n",
    "inputs = constant(borrower_features,dtype=float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(8,activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(6,activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "In chapter 2, you minimized a loss function with an optimizer. We'll revisit that here in the context of training neural networks. This entails finding the set of weights that corresponds to the minimum value of the loss.\n",
    "\n",
    "So what is a minimization problem? And what can go wrong when we try to solve one? Let's start with a simple thought experiment: you want to find the lowest point in the Grand Canyon, but all you can do is pick a point, measure the elevation, and then repeat the same to nearby points. This is what you do when you train a neural network: you pick a starting point, measure the loss, and then try to move to a lower loss. We will see how a common optimization algorithm, gradient descent, solves this problem.\n",
    "\n",
    "- Stochastic gradient descent (SGD) optimizer\n",
    "- RMS propagation optimizer\n",
    "- ADAM adaptive moment optimezer\n",
    "\n",
    "a coding example can be found below:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the model function\n",
    "def model(bias, weights, features = borrower_features):\n",
    "    product = tf.matmul(features, weights)\n",
    "    return tf.keras.activations.sigmoid(product,bias)\n",
    "\n",
    "def loss_function(bias, weights, targets = default, features = borrower_features):\n",
    "    predictions = model(bias, weights)\n",
    "    # Binary crossentropy, typical loss for binary classification problems\n",
    "    return tf.keras.losses.binary_crossentropy(targets, predictions)\n",
    "\n",
    "# Minimize the loss function with RMS propagation\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate = 0.01, momentum = 0.9)\n",
    "opt.minimize(lambda: loss_function(bias,weights), var_list = [bias, weights])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dangers of local minima\n",
    "Consider the plot of the following loss function, loss_function(), which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left.\n",
    "\n",
    "![](../../data/img/local_minima_dots_4_10.png)\n",
    "\n",
    "In this exercise, you will try to find the global minimum of loss_function() using keras.optimizers.SGD(). You will do this twice, each time with a different initial value of the input to loss_function(). \n",
    "\n",
    "- First, you will use x_1, which is a variable with an initial value of 6.0. \n",
    "- Second, you will use x_2, which is a variable with an initial value of 0.3. \n",
    "\n",
    "Note that loss_function() has been defined and is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.027515 0.25\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(6.0,float32)\n",
    "x_2 = Variable(0.3,float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def loss_function(x):\n",
    "    return 4.0*math.cos(x-1)+tf.divide(math.cos(2.0*np.pi*x),x)\n",
    "\n",
    "for j in range(100):\n",
    "    # Perform minimization using the loss function and x_1\n",
    "    opt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Perform minimization using the loss function and x_2\n",
    "    opt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding local minima\n",
    "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function().\n",
    "\n",
    "![](../../data/img/local_minima_dots_4_10.png)\n",
    "\n",
    "Several optimizers in tensorflow have a momentum parameter, including SGD and RMSprop. You will make use of RMSprop in this exercise. Note that x_1 and x_2 have been initialized to the same value this time. Furthermore, keras.optimizers.RMSprop() has also been imported for you from tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to see def of an function__\n",
    "```\n",
    "import inspect as i\n",
    "import sys\n",
    "sys.stdout.write(i.getsource(FUNCTION_NAME))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7445114 0.24999999\n"
     ]
    }
   ],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(0.05,float32)\n",
    "x_2 = Variable(0.05,float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(100):\n",
    "    opt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "    opt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Network in TensorFlow\n",
    "\n",
    "__Training a network in TensorFlow__\n",
    "In the final video in this chapter, we'll wrap-up by discussing important topics related to training neural networks in TensorFlow.\n",
    "\n",
    "__Initializing variables__  \n",
    "We often need to initialize hundreds or thousands of variables. Simply using ones will not work. And selecting initial values individually is tedious and infeasible in many cases. A natural alternative to this is to use random or algorithmic generation of initial values. \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define 500x500 random normal variable\n",
    "weights = tf.Variable(tf.random.normal([500,500]))\n",
    "\n",
    "# Define 500x500 truncated random normal variable\n",
    "weights = tf.Variable(tf.random.truncated_normal([500,500]))\n",
    "```\n",
    "or \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a dense layer the default initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu')\n",
    "\n",
    "# Define a dense layer the default zeros initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu', kernel_initializer = 'zeros')\n",
    "```\n",
    "\n",
    "We can, for instance, draw them from a probability distribution, such as the normal or uniform distributions. There are also specialized options, such as the Glorot initializers, which are designed for ML algorithms.\n",
    "\n",
    "__Neural networks and overfitting__  \n",
    "Overfitting is another important issue you'll encounter when training neural networks. Let's say you have a linear relationship between two variables. You decide to represent this relationship with a linear model, shown in red, and a more complex model, shown in blue. The complex model perfectly predicts the values in the training set, but performs worse in the test set. The complex model performed poorly because it overfit. __It simply memorized examples, rather than learning general patterns.__\n",
    "\n",
    "__Applying dropout__  \n",
    "A simple solution to the overfitting problem is to use dropout, an operation that will randomly drop the weights connected to certain nodes in a layer during the training process, as shown on the right. This will force your network to develop more robust rules for classification, since it cannot rely on any particular nodes being passed to an activation function. This will tend to improve out-of-sample performance.\n",
    "\n",
    "```python\n",
    "dropout1 = tf.keras.layers.Dropout(0.25)(<input>) # 25%: Fraction of the input units to drop.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization in TensorFlow\n",
    "A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level keras operations. We will also expand the set of input features from 3 to 23. Several operations have been imported from tensorflow: Variable(), random(), and ones()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import Variable,random, ones\n",
    "\n",
    "# Define the layer 1 weights\n",
    "w1 = Variable(random.normal([23, 7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = Variable(ones([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = Variable(random.normal([7,1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = Variable(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model and loss function\n",
    "In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as borrower_features and default. You defined the weights and biases in the previous exercise.\n",
    "\n",
    "Note that the predictions layer is defined as , where  is the sigmoid activation, layer1 is a tensor of nodes for the first hidden dense layer, w2 is a tensor of weights, and b2 is the bias tensor.\n",
    "\n",
    "The trainable variables are w1, b1, w2, and b2. Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
    "    # Apply dropout rate of 0.25\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(target, predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training neural networks with TensorFlow\n",
    "In the previous exercise, you defined a model, model(w1, b1, w2, b2, features), and a loss function, loss_function(w1, b1, w2, b2, features, targets), both of which are available to you in this exercise. \n",
    "\n",
    "You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of test_features and test_targets and is available to you. The trainable variables are w1, b1, w2, and b2. Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout().\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def confusion_matrix(default, model_predictions):\n",
    "\tdf = DataFrame(np.hstack([default, model_predictions.numpy() > 0.5]), columns = ['Actual','Predicted'])\n",
    "\tconfusion_matrix = crosstab(df['Actual'], df['Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\theatmap(confusion_matrix, cmap=\"Greys\", fmt=\"d\", annot=True, cbar=False)\n",
    "\tplt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model using test features\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Neural Networks with Keras\n",
    "In chapter 3, we saw how to define neural networks in TensorFlow, both using linear algebra and higher level Keras operations. In this lesson, we will introduce the Keras sequential API, and expand on our brief and informal introduction of the Keras functional API.\n",
    "\n",
    "## Sequential API\n",
    "A good way to construct this model in Keras is to use the sequential API. This API is simpler and makes strong assumptions about how you will construct your model. It assumes that you have an input layer, some number of hidden layers, and an output layer. All of these layers are ordered one after the other in a sequence.  \n",
    "\n",
    "- Input layer\n",
    "- Hidden layers\n",
    "- Output layer\n",
    "- Odered in sequence\n",
    "\n",
    "```python\n",
    "# Import tensorflow\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define a sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define first hidden layer\n",
    "model.add(\n",
    "    keras.layers.Dense(16, activation = 'relu', input_shape = (28*28,))\n",
    ")\n",
    "\n",
    "# Define second hidden layer\n",
    "model.add(\n",
    "    keras.layers.Dense(8, activation = 'relu')\n",
    ")\n",
    "\n",
    "# Define output layer\n",
    "model.add(\n",
    "    keras.layers.Dense(8, activation = 'softmax') # for probability\n",
    ")\n",
    "\n",
    "# The model is not ready yet, we must compile the model first and define the opt and loss  # first\n",
    "\n",
    "model.compile(\n",
    "    'adam', loss = 'categorical_crossentropy' # used to class. with more than 2 class\n",
    ")\n",
    "\n",
    "# Returns the schematics of the model\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "or as we did earlier:\n",
    "\n",
    "```python\n",
    "\n",
    "# Define layer 1 for model 2\n",
    "model2_layer1 = tf.keras.layers.Dense(units=8, activation='relu')(model2_inputs)\n",
    "\n",
    "# Define layer 2 for model 2\n",
    "model2_layer2 = tf.keras.layers.Dense(units=8, activation='relu')(model2_layer1)\n",
    "\n",
    "# Merge model 1 and model 2\n",
    "merged = tf.keras.layers.add([model1_layer2, model2_layer2])\n",
    "\n",
    "# Define a functional model\n",
    "model = tf.keras.Model(inputs=[model1_inputs, model2_inputs], outputs = merged)\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss = 'categorical_crossentropy')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sequential model in Keras\n",
    "In chapter 3, we used components of the keras API in tensorflow to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the keras sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the .summary() method to print the model's architecture, including the shape and number of parameters associated with each layer.\n",
    "\n",
    "Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that keras has been imported from tensorflow for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 12,732\n",
      "Trainable params: 12,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a Keras sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a sequential model\n",
    "In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the adam optimizer and the categorical_crossentropy loss. You will also use a method in keras to summarize your model's architecture. Note that keras has been imported from tensorflow for you and a sequential keras model has been defined as model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                272       \n",
      "=================================================================\n",
      "Total params: 13,628\n",
      "Trainable params: 13,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation = 'sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(16, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a multiple input model\n",
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the .summary() method to examine the joint model's architecture.\n",
    "\n",
    "Note that keras has been imported from tensorflow for you. Additionally, the input layers of the first and second models have been defined as m1_inputs and m2_inputs, respectively. Note that the two models have the same architecture, but one of them uses a sigmoid activation in the first layer and the other uses a relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation with Keras\n",
    "Earlier in the chapter, we defined neural networks in Keras. In this video, we will discuss how to train and evaluate them.  \n",
    "Whenever we train and evaluate a model in tensorflow, we typically use the same set of steps. \n",
    "1. we'll load and clean the data. \n",
    "2. we'll define a model, specifying an architecture. \n",
    "3. we'll train and validate the model. \n",
    "4. we perform evaluation.  \n",
    "\n",
    "## The fit() operation\n",
    "Fit operator looks as follows:\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    image_features, # trainings data\n",
    "    image_labels, # trainings label\n",
    "    batch_size = , # default 32\n",
    "    epochs = , # number of times \n",
    "    validation_split = , # the percentage of data used for validation\n",
    ")\n",
    "```\n",
    "\n",
    "Notice that we only supplied two arguments to fit: \n",
    "- features\n",
    "- labels. \n",
    "\n",
    "These are the only two required arguments. However, there are also many optional arguments, including: \n",
    "- batch_size \n",
    "- epochs\n",
    "- validation_split. \n",
    "\n",
    "We will cover each of these.\n",
    "\n",
    "### Batch Size and Epochs\n",
    "Let's start with the difference between the batch size and epochs parameters. The number of examples in each batch is the batch size, which is 32 by default. The number of times you train on the full set of batches is called the number of epochs. Here, the batch size is 5 and the number of epochs is 2. \n",
    "\n",
    "![batches](../../data/img/batches.png)\n",
    "\n",
    "Using multiple epochs allows the model to revisit the same batches, but with different model weights and possibly optimizer parameters, since they are updated after each batch.\n",
    "\n",
    "\n",
    "\n",
    "### Performing validation\n",
    "So what does the validation_split parameter do? It divides the dataset into two parts. The first part is the train set and the second part is the validation set.\n",
    "\n",
    "![validation](../../data/img/validation.png)\n",
    "\n",
    "Selecting a value of zero point two will put 20% of the data in the validation set.\n",
    "\n",
    "The benefit of using a validation split is that you can see how your model performs on both the data it was trained on, the training set, and a separate dataset it was not trained on, the validation set. Here, we can see the first 10 epochs of training. Notice that we can see the training loss and validation loss separately. __If the training loss becomes substantially lower than the validation loss, this is an indication that we're overfitting. We should either terminate the training process before that point or add regularization or dropout.__\n",
    "\n",
    "### Changing the metric\n",
    "Another benefit of the high level keras API is that we can swap less informative metrics, such as the loss, for ones that are easily interpretable, such as the share of accurately classified examples. We can do this by supplying accuracy to the metrics parameter of compile. We then apply fit to the model again with the same settings.\n",
    "\n",
    "```python\n",
    "# Recompile the model with the accuracy metric\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy', \n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Keras\n",
    "In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training.\n",
    "\n",
    "Note that keras has been imported from tensorflow for you. Additionally, the features are available as sign_language_features and the targets are available as sign_language_labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Define a sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define a hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('SGD', loss='categorical_crossentropy')\n",
    "\n",
    "# Complete the fitting operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=5)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and validation with Keras\n",
    "We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation.\n",
    "\n",
    "Note that keras has been imported for you from tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(32, activation = 'sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting detection\n",
    "In this exercise, we'll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples.\n",
    "\n",
    "You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting.\n",
    "\n",
    "Note that keras has been imported from tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(1024,activation = 'relu',input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Finish the model compilation\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Complete the model fit operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=50, validation_split=0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating models\n",
    "Two models have been trained and are available: large_model, which has many parameters; and small_model, which has fewer parameters. Both models have been trained using train_features and train_labels, which are available to you. A separate test set, which consists of test_features and test_labels, is also available.\n",
    "\n",
    "Your goal is to evaluate relative model performance and also determine whether either model exhibits signs of overfitting. You will do this by evaluating large_model and small_model on both the train and test sets. For each model, you can do this by applying the .evaluate(x, y) method to compute the loss for features x and labels y. You will then compare the four losses generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Evaluate the small model using the train data\n",
    "small_train = small_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the small model using the test data\n",
    "small_test = small_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Evaluate the large model using the train data\n",
    "large_train = large_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the large model using the test data\n",
    "large_test = large_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Print losses\n",
    "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
    "print('Large - Train: {}, Test: {}'.format(large_train, large_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models with the Estimators API\n",
    "The Estimators API is a high level TensorFlow submodule. Relative to the core, lower-level TensorFlow APIs and the high-level Keras API, model building in the Estimator API is less flexible. \n",
    "\n",
    "![Estimators API](../../data/img/estimators_api.png)\n",
    "\n",
    "This is because it enforces a set of best practices by placing restrictions on model architecture and training. The upside of using the Estimators API is that it allows for faster deployment. Models can be specified, trained, evaluated, and deployed with less code. Furthermore, there are many premade models that can be instantiated by setting a handful of model parameters.\n",
    "\n",
    "__Defining feature columns__\n",
    "\n",
    "```python\n",
    "# Import tensorflow under its standard alias\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a numeric feature column\n",
    "size = tf.feature_column.numeric_column('size')\n",
    "\n",
    "# Define a categorical feature column\n",
    "rooms = tf.feature_column.categorical_column_with_vocabulary_list('rooms',['1','2','3','4','5'])\n",
    "\n",
    "# Create feature column list\n",
    "features_list = [size,rooms]\n",
    "\n",
    "# Define a matrix feature column\n",
    "features_list = [tf.feature_column.numeric_column('image',shape=(784,))]\n",
    "\n",
    "```\n",
    "\n",
    "__Loading and Transforming Data__\n",
    "```python\n",
    "def input_fn():\n",
    "    # Define feature dictionary\n",
    "    features = {'size':[1340,1690,2720],'rooms':[1,3,4]}\n",
    "    # Define labels\n",
    "    labels = [221900,538000,180000]\n",
    "    return features,labels\n",
    "```\n",
    "\n",
    "__Define and train a regression__\n",
    "```python\n",
    "# Define a deep neural network regression - for continuous values\n",
    "model0 = tf.estimator.DNNRegressor(\n",
    "    feature_columns = feature_list, \n",
    "    hidden_units = [10,6,6,3] # Number of neurons in hidden layers\n",
    ")\n",
    "\n",
    "# Train the regression model\n",
    "model0.train(input_fn, steps = 20)\n",
    "\n",
    "```\n",
    "\n",
    "__Define and train a deep neural network__\n",
    "Alternatively, if one would like to create a classification model:\n",
    "```python\n",
    "# Define a deep neural network classifier - for classification only\n",
    "model1 = tf.estimator.DNNClassifier(\n",
    "    feature_columns = feature_list, \n",
    "    hidden_units = [10,6,6,3], # Number of neurons in hidden layers\n",
    "    n_classes = 4\n",
    ")\n",
    "\n",
    "# Train the regression model\n",
    "model0.train(input_fn, steps = 20)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to train with Estimators\n",
    "For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the estimator API.\n",
    "\n",
    "Rather than completing everything in one step, we'll break this procedure down into parts. \n",
    "- We'll begin by defining the feature columns and loading the data. \n",
    "- In the next exercise, we'll define and train a premade estimator. \n",
    "\n",
    "Note that feature_column has been imported for you from tensorflow. Additionally, numpy has been imported as np, and the Kings County housing dataset is available as a pandas DataFrame: housing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Define bedrooms and bathrooms\n",
    "bedrooms = np.array(housing['bedrooms'])\n",
    "bathrooms = np.array(housing['bathrooms'])\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "    # Define the labels\n",
    "    labels = np.array(housing['price'])\n",
    "    # Define the features\n",
    "    features = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing['bathrooms'])}\n",
    "    return features, labels\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Congratulations!  \n",
    "Congratulations! You've now completed this course on the fundamentals of the TensorFlow API in Python. In this final video, we'll review what you've learned, talk about two useful TensorFlow extensions, and then wrap-up with a discussion of the transition to TensorFlow two point zero.\n",
    "\n",
    "2. What you learned  \n",
    "In chapter 1, you learned low-level, basic, and advanced operations in TensorFlow. You learned how to define and manipulate variables and constants. You also learned the graph-based computational model that underlies TensorFlow and how it can be used to compute gradients and solve arbitrary optimization problems. In chapter 2, you learned how to load and transform data for use in your TensorFlow projects. You also saw how to use predefined and custom loss functions. We ended with a discussion of how to train models, and when and how to divide the training into batches.\n",
    "\n",
    "3. What you learned  \n",
    "In chapter 3, we moved on to training neural networks. You learned how to define neural network architecture in TensorFlow, both using low-level linear algebra operations and high-level Keras API operations. We talked about how to select activation functions and optimizers, and, ultimately, how to train models. In chapter 4, you learned how to make full use of the Keras API to train models in TensorFlow. We discussed the training and validation process and also introduced the high-level Estimators API, which can be used to streamline the production process.\n",
    "\n",
    "4. TensorFlow extensions  \n",
    "In addition to what we covered, there are also a two important TensorFlow extensions that did not fit into the course, but may be worthwhile to explore on your own. The first is TensorFlow Hub, which allows users to import pretrained models that can then be used to perform transfer learning. This will be particularly useful when you want to train an image classifier with a small number of images, but want to make use of a feature-extractor trained on a much larger set of different images. TensorFlow Probability is another exciting extension, which is also currently available as a standalone module. One benefit of using TensorFlow Probability is that it provides additional statistical distributions that can be used for random number generation. It also enables you to incorporate trainable statistical distributions into your models. Finally, TensorFlow Probability provides an extended set of optimizers that are commonly used in statistical research. This gives you additional tools beyond what the core TensorFlow module provides.\n",
    "\n",
    "1 Screenshot from https://tfhub.dev.  \n",
    "\n",
    "5. TensorFlow 2.0  \n",
    "Finally, I will say a few words about the difference between TensorFlow 2 and TensorFlow 1. If you primarily develop in 1, you may have noticed that you do not need to define static graphs or enable eager execution. This is done automatically in 2. Furthermore, TensorFlow 2 has substantially tighter integration with Keras. In fact, the core functionality of the TensorFlow 1 train module is handled by tf.Keras operations in 2. In addition to the centrality of Keras, the Estimators API also plays a more important role in TensorFlow 2. Finally, TensorFlow 2 also allows you to use static graphs, but they are available through the tf.function operation.\n",
    "\n",
    "1 Screenshot taken from https://www.tensorflow.org/guide/premade_estimators  \n",
    "6. Congratulations!  \n",
    "Congratulations! You've now completed the course and are ready to begin training your own models in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aef453b458c4ab5fa2ea65720376374b723672e2e6d783b41bc3ee1eca6104f5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
